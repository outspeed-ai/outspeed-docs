---
title: "Live Sports Commentator"
description: "A realtime commentator that provides live commentary on sports events."
---

import InstallOutspeedSnippet from "/snippets/install-outspeed.mdx";

<Info>
  Make sure you have gone over [QuickStart](/examples/quickstart) before trying
  this example.
</Info>

In this example, we will build a realtime commentator that provides commentary on a live poker tournament.
We accomplish this by using screen share to capture the live game on YouTube and microphone audio to interact with the commentator.

## **Setup Application with Sports Commentator in 4 Steps**

This app will process input from your microphone and video from screen share, send it to an LLM, and convert the response back to voice.

- The full code is available [here.](https://github.com/outspeed-ai/outspeed/blob/main/examples/sports_commentator/poker_commentator.py)

## **Steps**

<Steps>
  <Step title="Install Dependencies">
    <InstallOutspeedSnippet />
  </Step>

  <Step title="Create PokerCommentator Application">
    <Warning>
      Ensure that you've followed the steps from [QuickStart](/examples/quickstart) and navigate to the base directory of the cloned outspeed repository.
    </Warning>
    Create a file `poker_commentator.py` and add the following code:
    ```python
    import outspeed as sp
    
    @sp.App()
    class PokerCommentator:
        async def setup(self):
            self.deepgram_node = sp.DeepgramSTT()
            self.keyframe_node = sp.KeyFrameDetector(key_frame_threshold=0.8, key_frame_max_time=15)
            self.llm_node = sp.OpenAIVision(
                system_prompt="You are a poker commentator. Use exclamation points to show excitement. Keep it high level. Keep the response short and concise. No special characters.",
                temperature=0.9,
                store_image_history=False,
                model="gpt-4o-mini",
                max_output_tokens=50,
            )
            self.token_aggregator_node = sp.TokenAggregator()
            self.tts_node = sp.CartesiaTTS(voice_id="5619d38c-cf51-4d8e-9575-48f61a280413")
            self.vad_node = sp.SileroVAD(min_volume=0, min_speech_duration_seconds=0.2)
    
        @sp.streaming_endpoint()
        async def run(self, audio_input_stream: sp.AudioStream, video_input_stream: sp.VideoStream):
            deepgram_stream: sp.TextStream = self.deepgram_node.run(audio_input_stream)
            vad_stream: sp.VADStream = self.vad_node.run(audio_input_stream.clone())
    
            key_frame_stream: sp.VideoStream = self.keyframe_node.run(video_input_stream)
    
            llm_input_stream = sp.merge([deepgram_stream, key_frame_stream])
    
            llm_token_stream: sp.TextStream
            chat_history_stream: sp.TextStream
            llm_token_stream, chat_history_stream = self.llm_node.run(llm_input_stream)
    
            token_aggregator_stream: sp.TextStream = self.token_aggregator_node.run(llm_token_stream)
    
            tts_stream: sp.AudioStream = self.tts_node.run(token_aggregator_stream)
    
            video_output_stream: sp.VideoStream = video_input_stream.clone()
    
            self.llm_node.set_interrupt_stream(vad_stream)
            self.token_aggregator_node.set_interrupt_stream(vad_stream.clone())
            self.tts_node.set_interrupt_stream(vad_stream.clone())
            self.keyframe_node.set_interrupt_stream(vad_stream.clone())
    
            return tts_stream, chat_history_stream, video_output_stream
    
        async def teardown(self):
            await self.deepgram_node.close()
            await self.keyframe_node.close()
            await self.llm_node.close()
            await self.token_aggregator_node.close()
            await self.tts_node.close()
    
    if __name__ == "__main__":
        PokerCommentator().start()
    ```
  </Step>

  <Step title="Setup API Keys and Run">
    <Tabs>

      <Tab title="Local Server">
        You will need the following environment variables:

        1. `DEEPGRAM_API_KEY` - Obtain from the [Deepgram Console](https://console.deepgram.com/) under the API key tab.
        2. `OPENAI_API_KEY` - Get this from the [OpenAI Developer Portal](https://platform.openai.com/api-keys) in the API keys section.
        3. `CARTESIA_API_KEY` - Retrieve from [Cartesia Keys](https://play.cartesia.ai/keys) under the API key tab.

        All providers offer a free tier. Once you have your keys, create a `.env` file in the same directory as `poker_commentator.py` and add the following:

        ```bash
        DEEPGRAM_API_KEY=<your_deepgram_api_key>
        OPENAI_API_KEY=<your_openai_api_key>
        CARTESIA_API_KEY=<your_cartesia_api_key>
        ```

        Finally, run the server with:

        ```bash
        python poker_commentator.py
        ```

        The console will output the URL to connect to the server (default is http://localhost:8080), which is your Outspeed App URL.
      </Tab>

      <Tab title="Outspeed Cloud">
        <Info>
          Ensure you have an Outspeed API key before deploying. To obtain one, [Contact us](mailto:contact@outspeed.com).
        </Info>
        Deploy using the outspeed [Python SDK](https://github.com/outspeed-ai/outspeed.git):

        ```bash
        outspeed deploy --api-key=<your-api-key> poker_commentator.py
        ```

        After deployment, you'll receive a link to your deployed function on the Outspeed server. Copy this link.

        (Coming soon) Alternatively, access your app via the Outspeed dashboard to retrieve the link.
      </Tab>

    </Tabs>
  </Step>

  <Step title="Run Demo">
    We have created a simple frontend using our [React SDK](/javascript/introduction).

    1. Open a YouTube video/stream of a poker tournament. Ensure the audio is muted.
       [Example Video](https://youtu.be/3wXxz5E2IiY?si=6ZyT3EdwivMK1FKM&t=117)
    2. In another tab, visit the [Playground](https://playground.outspeed.com/sports_commentator_input).
    3. Enter your Outspeed App URL (or localhost URL if running locally) in the URL field.
    4. Select your audio device and set the desired resolution (higher resolution works better). Click **Share Screen** to begin.
    5. When prompted, share your screen by selecting the YouTube tab.
    6. Set the YouTube video to **full screen** for accurate commentary.
  </Step>
</Steps>

## **Understanding the Process**

Review the code in `poker_commentator.py`.

#### **Setup**

The `PokerCommentator` class initializes with the `setup` method, which is automatically called when the application starts. This method sets up the necessary services and loads models:

- **DeepgramSTT**: Converts spoken audio to text using Deepgram's speech-to-text API, configured with a sample rate of 8000 Hz.
- **KeyFrameDetector**: Analyzes video streams to detect significant moments, using a sensitivity threshold of 0.8 and a maximum time interval of 15 seconds between key frames.
- **OpenAIVision**: Processes audio and video inputs to generate insightful poker commentary, guided by a detailed system prompt. It operates with a response temperature of 0.9 and maintains a chat history for context.
- **TokenAggregator**: Aggregates tokens from OpenAIVision to form coherent responses.
- **CartesiaTTS**: Converts text responses back into spoken audio using Cartesia's API, optimized for low latency and using a specific voice model.
- **SileroVAD**: Detects voice activity to manage interruptions in the commentary.

#### **Streaming Endpoint**

The `run` method in the `PokerCommentator` class is marked as a streaming endpoint, handling real-time audio and video streams. It processes these streams as follows:

1. **Audio Processing**: Converts the audio input stream to text using the `DeepgramSTT` service.
2. **Voice Activity Detection**: Analyzes the audio stream to detect speech segments using `SileroVAD`.
3. **Video Processing**: Analyzes the video input stream to identify key moments using the `KeyFrameDetector`.
4. **Commentary Generation**: Merges the text from Deepgram and the video analysis to generate commentary using `OpenAIVision`.
5. **Token Aggregation**: Refines the generated commentary tokens for coherence using the `TokenAggregator`.
6. **Text-to-Speech**: Converts the aggregated text into spoken audio using `CartesiaTTS`.
7. **Stream Management**: Clones the video input stream for output and sets interrupt streams based on voice activity detection to manage real-time interactions.

The method outputs three streams: the audio stream of the commentary, the chat history text stream, and the cloned video input stream.

## **Support**

For any assistance or questions, feel free to join our [Discord community](https://discord.gg/cmfFw6SYvp). We're excited to see what you build!